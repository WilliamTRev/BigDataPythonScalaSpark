//Ref:
https://www.datasciencebytes.com/bytes/2016/04/18/getting-started-with-spark-running-a-simple-spark-job-in-java/

//Open a new project:
IntelliJ > File > New > Project > Maven > seelct the Java SDK > Next


//Add the below info:
Name = Spark-Java
GroupID = com.rev
ArtifactId = Spark-Java

//Then
Click Finish


//Open IntelliJ Preferences and make sure "Detect compiler automatically", and "Automatically download: |x| Sources |x| Documentation" are checked under:

File > Settings > Build, Execution, Deployment -> Build Tools -> Maven -> Importing

//This tells IntelliJ to download any dependencies we need.

Apply > OK


// Add the following snippet to pom.xml, above the </project> tag.
// Ref: https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.12/3.1.1
<dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>3.1.1</version>
        </dependency>

// This tells Maven that our code depends on Spark and to bundle Spark in our project.

// Writing our application:
// Select the "java" folder on IntelliJ's project menu (on the left), right click and select New -> Java Class. Name this class SparkAppJavaTest1.
Project > Saprk-Java > src > main > java > right click and select New -> Java Class. Name this class SparkAppJavaTest1.

// To make sure everything is working, paste the following code into the SparkAppJavaTest1 class 
public class SparkAppJavaTest1 {
    public static void main(String[] args) {
        System.out.println("Hello World");
    }
}

// Run the class (Run -> Run... in IntelliJ's menu bar).
Run > Run > SparkAppJavaTest1 

Result:
Hello World
Process finished with exit code 0


---------------------------------------------------
Program2: WordCount

Project > Spark-java > src > main > java > right click and select New -> Java Class. Name this class SparkAppJavaTest2.

// Code:
import java.util.Arrays;
import java.util.Iterator;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;

import scala.Tuple2;


public class WordCount {

    /*For Simplicity,
     *We are creating custom Split function,so it makes code easier to understand
     *We are implementing FlatMapFunction interface.*/
    static class SplitFunction implements FlatMapFunction<String, String>
    {

        private static final long serialVersionUID = 1L;

        @Override
        public Iterator<String> call(String s) {
            return Arrays.asList(s.split(" ")).iterator();
        }

    }

    public static void main(String[] args)
    {

        SparkConf sparkConf = new SparkConf();

        sparkConf.setAppName("Spark WordCount example using Java");

        //Setting Master for running it from IDE.
        sparkConf.setMaster("local[1]");

        JavaSparkContext sparkContext = new JavaSparkContext(sparkConf);

        /*Reading input file whose path was specified as args[0]*/
        JavaRDD<String> textFile = sparkContext.textFile(args[0]);

        /*Creating RDD of words from each line of input file*/
        JavaRDD<String> words = textFile.flatMap(new SplitFunction());

        /*Below code generates Pair of Word with count as one
         *similar to Mapper in Hadoop MapReduce*/
        JavaPairRDD<String, Integer> pairs = words
                .mapToPair(new PairFunction<String, String, Integer>() {
                    public Tuple2<String, Integer> call(String s) {
                        return new Tuple2<String, Integer>(s, 1);
                    }
                });

        /*Below code aggregates Pairs of Same Words with count
         *similar to Reducer in Hadoop MapReduce
         */
        JavaPairRDD<String, Integer> counts = pairs.reduceByKey(
                new Function2<Integer, Integer, Integer>() {
                    public Integer call(Integer a, Integer b) {
                        return a + b;
                    }
                });

        /*Saving the result file to the location that we have specified as args[1]*/
        counts.saveAsTextFile(args[1]);
        sparkContext.stop();
        sparkContext.close();
    }
}
---------------------------------------------------------------------------------------

// Create an input folder under Spark-Java and place a .txt file (to be used for word count) with data inside the folder
\Spark-Java\input\WordCountFile.txt


// Edit Configurations:
Run > Edit Configurations

//Add input and output location in the program arguments
Program Arguments = input/WordCountFile.txt output/WordCountOp

//Run
Run > Run 'WordCount'

Result:

Process finished with exit code 0


//Check the output in the output folder that got created.

Note: If the program needs to be Run again, then delete the output folder before the rerun

