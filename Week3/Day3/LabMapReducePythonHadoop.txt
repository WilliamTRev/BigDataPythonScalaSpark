https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/

1. cd ~

2. (optional, Thank you Zach) sudo apt install python-is-python3

3. touch mapper.py

3. vim mapper.py (and paste below)

#!/usr/bin/env python
"""mapper.py"""

import sys

# input comes from STDIN (standard input)
for line in sys.stdin:
    # remove leading and trailing whitespace
    line = line.strip()
    # split the line into words
    words = line.split()
    # increase counters
    for word in words:
        # write the results to STDOUT (standard output);
        # what we output here will be the input for the
        # Reduce step, i.e. the input for reducer.py
        #
        # tab-delimited; the trivial word count is 1
        print ('%s\t%s' % (word, 1))

4. touch reducer.py

5. vim reducer.py (and paste below)

#!/usr/bin/env python
"""reducer.py"""

from operator import itemgetter
import sys

current_word = None
current_count = 0
word = None

# input comes from STDIN
for line in sys.stdin:
    # remove leading and trailing whitespace
    line = line.strip()

    # parse the input we got from mapper.py
    word, count = line.split('\t', 1)

    # convert count (currently a string) to int
    try:
        count = int(count)
    except ValueError:
        # count was not a number, so silently
        # ignore/discard this line
        continue

    # this IF-switch only works because Hadoop sorts map output
    # by key (here: word) before it is passed to the reducer
    if current_word == word:
        current_count += count
    else:
        if current_word:
            # write result to STDOUT
            print ('%s\t%s' % (current_word, current_count))
        current_count = count
        current_word = word

# do not forget to output the last word if needed!
if current_word == word:
    print ('%s\t%s' % (current_word, current_count))

6. echo "foo foo quux labs foo bar quux" | ~/mapper.py

7. echo "foo foo quux labs foo bar quux" | ~/mapper.py | sort -k1,1 | ~/reducer.py

8. cat > MR.txt (and paste below)
foo foo quux labs foo bar quux

9. cat MR.txt | ~/mapper.py | sort -k1,1 | ~/reducer.py

10. $HADOOP_HOME/sbin/start-dfs.sh (may need to do "sudo service ssh restart" first)
11. $HADOOP_HOME/sbin/start-yarn.sh

12. hdfs dfs -mkdir -p /user/test
13. hdfs dfs -put MR.txt /user/test
(check with hdfs dfs -ls /user/test or hdfs dfs -cat /user/test/MR.txt)

14. find . -name "*hadoop-streaming*"
(should find something like ./hadoop/hadoop-3.3.3/share/hadoop/tools/lib/hadoop-streaming-3.3.3.jar)

15. cd $HADOOP_HOME/bin

16. hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.3.jar \
-file ~/mapper.py    -mapper ~/mapper.py \
-file ~/reducer.py   -reducer ~/reducer.py \
-input /user/test/*  -output /user/test/output

(make sure output directory is not created yet, if it is delete it before running above)

17. hdfs dfs -cat /user/test/output/*
(should see desired output from wordcount)
